{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## mount google drive"
      ],
      "metadata": {
        "id": "qpmcxBHL9ZSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELQmAAeCERqh",
        "outputId": "f3eea1ed-f4ca-430f-cd37-5f4a35fa8aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change to working directory"
      ],
      "metadata": {
        "id": "rfRt40s0_1i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/Othercomputers/My MacBook Pro/dvlp/dvlp\"\n",
        "%cd \"/content/drive/Othercomputers/My MacBook Pro/dvlp/dvlp\"\n",
        "# %env PYTHONPATH=\"/env/python:/content/drive/Othercomputers/My MacBook Pro/dvlp/dvlp\"\n",
        "\n",
        "pypath = \"/content/drive/Othercomputers/My MacBook Pro/dvlp/dvlp\"\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/drive/Othercomputers/My MacBook Pro/dvlp/dvlp\"\n",
        "\n",
        "sys.path.append(pypath)\n",
        "!echo $PYTHONPATH\n",
        "!ls "
      ],
      "metadata": {
        "id": "0QqX6S1J8iAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "i5IW-DbuEmh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls datasets\n",
        "import sys\n",
        "print(sys.version)\n",
        "print(sys.path)"
      ],
      "metadata": {
        "id": "VEQktsMfKezd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from importlib.util import find_spec\n",
        "\n"
      ],
      "metadata": {
        "id": "T-efyQ7dKB_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.emnist import EMNIST"
      ],
      "metadata": {
        "id": "ecdOKjP6NaIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = EMNIST()\n",
        "data.prepare_data()\n",
        "data.setup()\n",
        "print(data)"
      ],
      "metadata": {
        "id": "cRG3JXFx9WpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWzeSzy68eLJ"
      },
      "source": [
        "# Implementation of code in book in python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI4O1ok-8eLL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from typing import Callable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCV2mXa8eLN"
      },
      "source": [
        "# Resources\n",
        "- [this colab]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev4CFiQB8eLO"
      },
      "source": [
        "# Building a simple model without using any pytorch nn module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfBzjvkK8eLO"
      },
      "outputs": [],
      "source": [
        "# compare to https://colab.research.google.com/drive/1HS3qbHArkqFlImT2KnF5pcMCz7ueHNvY?usp=sharing\n",
        "class Linear:\n",
        "    def __init__(self, n_in, n_out):\n",
        "        self.n_in = n_in\n",
        "        self.b = torch.zeros(n_out)\n",
        "\n",
        "        # Note that in some of the textbooks and lectures, the weight matrix w is defined in reversed order so \n",
        "        # the multiplication should also be rwversed \n",
        "        # initialization is important to prevent weights going to zero: this is known as Kaiming / he initializatiom\n",
        "        self.w = torch.randn(n_in, n_out) * np.sqrt(2. / n_in)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.x = x    # stored fo backprop\n",
        "        x_out = x @ self.w  + self.b    \n",
        "        return x_out\n",
        "\n",
        "    def backward(self, grad):\n",
        "        self.grad_w = self.x.T @ grad\n",
        "        self.grad_b = grad.sum(axis=0)\n",
        "        self.x_grad = grad @ self.w.T\n",
        "        return self.x_grad\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.w = self.w - lr * self.grad_w\n",
        "        self.b = self.b - lr * self.grad_b\n",
        "\n",
        "class Relu:\n",
        "    def __call__(self, x):\n",
        "        self.x = x\n",
        "        self.output = x.clamp_min(0.0)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, grad):\n",
        "      self.grad = (self.x > 0) * grad\n",
        "      return self.grad\n",
        "\n",
        "class MSE:\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        # save for backprop setp\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "        return ((y_pred-y_true)**2).mean()\n",
        "\n",
        "    def backward(self):\n",
        "        self.grad = 2 * (self.y_pred-self.y_true) / len(self.y_pred)\n",
        "        return self.grad\n",
        "\n",
        "\n",
        "class Model:\n",
        "  def __init__(self, input_dim, num_hidden):\n",
        "    self.linear1 = Linear(input_dim, num_hidden)\n",
        "    self.relu = Relu()\n",
        "    self.linear2 = Linear(num_hidden, 1)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    l1 = self.linear1(x)\n",
        "    r = self.relu(l1)\n",
        "    l2 = self.linear2(r)\n",
        "    return l2\n",
        "  \n",
        "  def backward(self, output_gradient):\n",
        "    linear2_gradient = self.linear2.backward(output_gradient)\n",
        "    relu_gradient = self.relu.backward(linear2_gradient)\n",
        "    linear1_gradient = self.linear1.backward(relu_gradient)\n",
        "    # print('Model backward', linear2_gradient.shape, relu_gradient.shape, linear1_gradient.shape)\n",
        "    # import pdb; pdb.set_trace()\n",
        "    return linear1_gradient\n",
        "\n",
        "  def update(self, lr):\n",
        "    self.linear2.update(lr)\n",
        "    self.linear1.update(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bfd85BK8eLP"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def fit(x: np.ndarray, y: np.ndarray, model: Callable, loss: Callable, lr: float, num_epochs: int):\n",
        "  for epoch in range(num_epochs):\n",
        "    y_pred = model(x)\n",
        "    loss_value = loss(y_pred, y)\n",
        "    print(f'Epoch {epoch}, loss {loss_value}')\n",
        "    gradient_from_loss = loss.backward()\n",
        "    model.backward(gradient_from_loss)\n",
        "    model.update(lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c69h-tvP8eLP"
      },
      "source": [
        "## Test the linear layer\n",
        "For more details, please consider consulting [this colab](https://colab.research.google.com/drive/1HS3qbHArkqFlImT2KnF5pcMCz7ueHNvY?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfXW5bdR8eLQ"
      },
      "outputs": [],
      "source": [
        "n = 50\n",
        "d = 1\n",
        "x = torch.rand(n, d)\n",
        "print(x.shape)\n",
        "\n",
        "# y = 5x + 10\n",
        "weights_true = torch.Tensor(np.array([[5],]))\n",
        "bias_true = torch.Tensor(np.array([10]))\n",
        "\n",
        "print(weights_true.shape)\n",
        "y_true = x @ weights_true + bias_true\n",
        "print(f'x: {x.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y: {y_true.shape}')\n",
        "\n",
        "linear_layer = Linear(d,1)\n",
        "loss = MSE()\n",
        "\n",
        "y_pred = linear_layer(x)\n",
        "\n",
        "print(linear_layer.w)\n",
        "print(f'Loss: {loss(y_pred,y_true)} grad={loss.backward().norm()}')\n",
        "print(f'mean: {y_pred.mean()}')\n",
        "print(f'x: {x.shape}, weights: {linear_layer.w.shape}, bias: {linear_layer.b.shape}, y: {y_true.shape}')\n",
        "\n",
        "plt.plot(x, y_true, marker='x', label='underlying function')\n",
        "plt.scatter(x, y_pred, color='r', marker='.', label='our function')\n",
        "plt.legend()\n",
        "\n",
        "y_pred = linear_layer(x)\n",
        "print(f'Initial loss: {loss(y_pred, y_true)}')\n",
        "loss_gradient = loss.backward()\n",
        "linear_layer.backward(loss_gradient)\n",
        "linear_layer.update(0.1)\n",
        "y_pred = linear_layer(x)\n",
        "print(f'Final loss: {loss(y_pred, y_true)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD_nl4Pu8eLS"
      },
      "source": [
        "## Test A simple model with one hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWw0y6a-8eLS"
      },
      "outputs": [],
      "source": [
        "loss = MSE()\n",
        "model = Model(d, 10)\n",
        "y_pred = model(x)\n",
        "loss_value = loss(y_pred, y_true)\n",
        "print(loss_value)\n",
        "loss_gradient = loss.backward()\n",
        "model.backward(loss_gradient)\n",
        "model.update(0.01)\n",
        "y_pred = model(x)\n",
        "loss_value = loss(y_pred, y_true)\n",
        "print(loss_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV45p_de8eLS"
      },
      "source": [
        "## Add uniform random noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjxRThDp8eLT"
      },
      "outputs": [],
      "source": [
        "n = 50\n",
        "d = 1\n",
        "variance = 0.8\n",
        "x_train = torch.linspace(0.0, 1.0, n)[:,None]\n",
        "\n",
        "# y = 5x + 10\n",
        "weights_true = torch.tensor(np.array([[5],], dtype=np.float32))\n",
        "bias_true = torch.tensor(np.array([10], dtype=np.float32))\n",
        "noise = (torch.rand(n)-0.5)*np.sqrt(variance)\n",
        "print(noise.shape)\n",
        "\n",
        "y_true = x_train @ weights_true + bias_true\n",
        "y_train = y_true + noise[:,None]\n",
        "print(f'x: {x.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y_true: {y_true.shape}, y_train: {y_train.shape}')\n",
        "\n",
        "plt.plot(x_train, y_true, marker='x', label='true function')\n",
        "plt.scatter(x_train, y_train, color='r', marker='.', label='data with noise')\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckIG05dD8eLT"
      },
      "source": [
        "## .. and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cedqeqov8eLT"
      },
      "outputs": [],
      "source": [
        "loss = MSE()\n",
        "model = Model(d, 10)\n",
        "fit(x_train, y_train, model=model, loss=loss, lr=0.001, num_epochs=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jADYwv6c8eLT"
      },
      "outputs": [],
      "source": [
        "n_pred = 155\n",
        "x_pred = torch.linspace(0.0,1.0,n_pred)[:,None]\n",
        "y_pred = model(x_pred)\n",
        "\n",
        "plt.plot(x_train, y_true, label='true function')\n",
        "plt.scatter(x_train, y_train, color='r', marker='.', label='data with noise')\n",
        "plt.scatter(x_pred, y_pred, color='g', marker='x', label='predictions')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjwkIoVg8eLU"
      },
      "source": [
        "# What's up with those gradients?\n",
        "The code is ugly as we have to store information for gradients that is used only for training. Pyttorch and other frameworks are based of the concept of `automatic differntiation` ehich handles all of those nasties for you. Pytorch uses the [auto-grad](https://pytorch.org/docs/stable/autograd.html) package for that. For more info look at the [PyTorch Autograd bolg](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95).\n",
        "\n",
        "So if yo consider Pytorch [Linear model](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear), it looks a lot like the code we wrote above except that the `weight` and `bias` tensors are now wrapped in [Parameter](https://pytorch.org/docs/1.9.1/_modules/torch/nn/parameter.html#Parameter) which, among other things, set the flag `requires_grad` to True, thus allowing auto-grad to track gradients. The gradients stored in the `grad` member.\n",
        "\n",
        "## Optimization for inference\n",
        "See the [A GENTLE INTRODUCTION TO TORCH.AUTOGRAD](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) and the [PERFORMANCE TUNING GUIDE](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgvq8J-c8eLU"
      },
      "outputs": [],
      "source": [
        "print(model.linear1.w.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LlUKCQo8eLU"
      },
      "source": [
        "# Writing the model in the torch way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG74ON0n8eLU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TwoLayers(nn.Module):\n",
        "  def __init__(self, input_dim, num_hidden):\n",
        "    super().__init__()\n",
        "    self.name = 'TwoLayers'\n",
        "\n",
        "    self.linear1 = nn.Linear(input_dim, num_hidden)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(num_hidden, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    l1 = self.linear1(x)\n",
        "    r = self.relu(l1)\n",
        "    l2 = self.linear2(r)\n",
        "    return l2\n",
        "\n",
        "class ThreeLayers(nn.Module):\n",
        "  def __init__(self, input_dim, nh1, nh2):\n",
        "    super().__init__()\n",
        "    self.name = 'ThreeLayers'\n",
        "    self.linear1 = nn.Linear(input_dim, nh1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(nh1, nh2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.linear3 = nn.Linear(nh2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    l1 = self.linear1(x)\n",
        "    r1 = self.relu1(l1)\n",
        "    l2 = self.linear2(r1)\n",
        "    r2 = self.relu2(l2)\n",
        "    l3 = self.linear3(r2)\n",
        "    return l3\n",
        "\n",
        "def many_layers(sizes: list):\n",
        "    layers = []\n",
        "    first = sizes[0]\n",
        "    for s in sizes[1:]:\n",
        "      layers.append(nn.Linear(first, s))\n",
        "      layers.append(nn.ReLU())\n",
        "      first = s\n",
        "    layers.append(nn.Linear(sizes[-1], 1))\n",
        "    net = nn.Sequential(*layers)\n",
        "    return net\n",
        "\n",
        "def torch_fit(x_train: np.ndarray, y_train: np.ndarray, model: Callable, loss: Callable, lr: float, num_epochs: int, print_every_n_steps: int):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred_tensor = model(x_train)\n",
        "    loss_value = loss(y_pred_tensor, y_train)\n",
        "    if epoch == 0 or epoch == 100 or epoch == 1000 or epoch == 5000 or epoch == 9500: print(f'Epocj: {epoch} loss={loss_value}')\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "  print(f'Epocj: {epoch} loss={loss_value}')\n",
        "\n",
        "def count_params(model, model_name):\n",
        "  all_params = sum(p.numel() for p in model.parameters())\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f'Model has total of {all_params}, trainable parameters: {trainable_params}')\n",
        "\n",
        "\n",
        "def generate_data(model_complexity=1):\n",
        "  n = 50\n",
        "  x_train = torch.linspace(0.0, 1.0, n)[:,None]\n",
        "  if model_complexity==1:\n",
        "    # y = 5x + 10\n",
        "    variance = 1.8\n",
        "    noise = (torch.rand(n)-0.5)*np.sqrt(variance)\n",
        "    weights_true = torch.tensor(np.array([[5],], dtype=np.float32))\n",
        "    bias_true = torch.tensor(np.array([10], dtype=np.float32))\n",
        "    y_true = x_train @ weights_true + bias_true\n",
        "  elif model_complexity == 2:\n",
        "    # y = x**2 + 0.5x + 0.3\n",
        "    variance = 0.1\n",
        "    noise = (torch.rand(n)-0.5)*np.sqrt(variance)\n",
        "\n",
        "    x_train = torch.linspace(0.0, 1.0, n)[:,None]\n",
        "    weights_true = torch.tensor(np.array([[0.5],], dtype=np.float32))\n",
        "    bias_true = torch.tensor(np.array([10], dtype=np.float32))\n",
        "    y_true = (x_train-0.5)**2 + x_train @ weights_true + bias_true\n",
        "  else:\n",
        "    # y = sin(x) + 0.3\n",
        "    variance = 0.1\n",
        "    noise = (torch.rand(n)-0.5)*np.sqrt(variance)\n",
        "\n",
        "    x_train = torch.linspace(0.0, 1.0, n)[:,None]*3.14159*2.\n",
        "    weights_true = torch.tensor(np.array([[0.5],], dtype=np.float32))\n",
        "    bias_true = torch.tensor(np.array([10], dtype=np.float32))\n",
        "    y_true = torch.sin(x_train) + bias_true\n",
        "    x_train = x_train / torch.max(x_train)\n",
        "\n",
        "  y_train = y_true + noise[:,None]\n",
        "  print(f'x_train: {x_train.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y_true: {y_true.shape}, y_train: {y_train.shape}')\n",
        "\n",
        "  return x_train, y_train, y_true\n",
        "\n",
        "\n",
        "def draw_data(x_train, y_train, y_true):\n",
        "  plt.scatter(x_train, y_true, marker='x', label='true function')\n",
        "  plt.scatter(x_train, y_train, color='r', marker='.', label='data with noise')\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "def draw_results(model, m_name, x_train, y_train, y_true, n_pred=10):\n",
        "  print(f'Drawe reuslts of model: {m_name}')\n",
        "  x_pred = torch.linspace(0.0,1.0,n_pred)[:,None]\n",
        "  y_pred = model(x_pred)\n",
        "\n",
        "  # Note that y_pred have gradient information, thus we will not be able to display it \n",
        "  # In the display function we will use detach to remove the gradient info and allow converting to numpy.\n",
        "  print(f'Predicting {len(x_pred)} points. y_pred-require-grads={y_pred.requires_grad}')\n",
        "\n",
        "  plt.plot(x_train, y_true, label='true function')\n",
        "  plt.scatter(x_train, y_train, color='r', marker='.', label='data with noise')\n",
        "  plt.scatter(x_pred, y_pred.detach(), color='g', marker='x', label='predictions')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifz3M-cn8eLV"
      },
      "source": [
        "# Generating the data\n",
        "Often will you read in articals teh term `data distribution`. This relates to our perceptual model of the data generation process. We think of a known phenomenom from which we sample the data that we actualy use fo training. Below is out data generation process: We generate the data from three functions corrupted by noise:\n",
        "- For `model_complexity=1`, we sample from a linear function\n",
        "- For `model_complexity=2`, we sample from a quadratic function\n",
        "- For `model_complexity=3`, we sample from a sine function\n",
        "\n",
        "Of course, in the real world we get only the data and the unxderlying function is only in our imagination. Often we will imagine incorrectly and we need be aware of that and check ourselves repeatidly. \n",
        "\n",
        "## Be aware of abuses of referring to those data distribution stuff\n",
        "See [this tweet](https://twitter.com/yoavgo/status/1481705515122143240?s=27)  by [Yoav Goldberg](https://u.cs.biu.ac.il/~yogo/). People often abuse terms related tp `data distributions` so do not let yourself intimidated by BS written in papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZpfyPGH8eLV"
      },
      "outputs": [],
      "source": [
        "# play with the model complexity = 1,2,3\n",
        "model_complexity = 3\n",
        "x_train, y_train, y_true = generate_data(model_complexity=model_complexity)\n",
        "draw_data(x_train, y_train, y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPTZWya-8eLV"
      },
      "outputs": [],
      "source": [
        "loss = nn.MSELoss()\n",
        "model_2 = TwoLayers(1, 10)\n",
        "count_params(model_2, 'model_2')\n",
        "print(model_2.linear1.weight.requires_grad)\n",
        "\n",
        "# try learning rate 01, 0.01, 0001\n",
        "torch_fit(x_train, y_train, model=model_2, loss=loss, lr=0.001, num_epochs=5000,print_every_n_steps=100)\n",
        "draw_results(model_2, 'model_2', x_train, y_train, y_true, n_pred=60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXwj0Kyo8eLW"
      },
      "outputs": [],
      "source": [
        "loss = nn.MSELoss()\n",
        "model_3 = ThreeLayers(1, 20, 10)\n",
        "count_params(model_3, 'model_3')\n",
        "\n",
        "# Try learning rates: 0.1, 0.01, 0.001,\n",
        "torch_fit(x_train, y_train, model=model_3, loss=loss, lr=0.01, num_epochs=5000,print_every_n_steps=100)\n",
        "draw_results(model_3, 'model_3', x_train, y_train, y_true, n_pred=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_8F9_NF8eLW"
      },
      "outputs": [],
      "source": [
        "x_train, y_train, y_true = generate_data(model_complexity=model_complexity)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "model_many = many_layers(sizes=[1,100,50,40,30,20,10])\n",
        "\n",
        "count_params(model_many, 'model_many')\n",
        "\n",
        "# Try learning rate 0.1, 0.01, 0.001, 0.005\n",
        "torch_fit(x_train, y_train, model=model_many, loss=loss, lr=0.005, num_epochs=5000, print_every_n_steps=100)\n",
        "draw_results(model_many, 'model_many', x_train, y_train, y_true, n_pred=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwJIOkRs8eLW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b490ba79cd6e0cad5d561930ddcb592f66007d6828f74aef7cec19c0ea8e73d4"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('dvlp_m1': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Copy of colab_play.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}