{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import platform\n",
    "print(platform.platform())\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html) course by [sebastian raschka](https://sebastianraschka.com/). Videos\n",
    "- [L19.1 Sequence Generation with Word and Character RNNs](https://www.youtube.com/watch?v=fSBw6TrePPg&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=155). INtroducing character RNN with s imple toy example\n",
    "- [L19.2.1 Implementing a Character RNN in PyTorch (Concepts)](https://www.youtube.com/watch?v=PFcWQkGP4lU&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=156). Discuss the LSTM class\n",
    "- []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM in pytorch\n",
    "See [the video](https://www.youtube.com/watch?v=PFcWQkGP4lU&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=156) abd [slides](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "input_size = 10      # The dimension of the input vector x, here number of characters in string\n",
    "hidden_size = 20                        # The dimension of the embedding layer \n",
    "num_lstm_layers = 2                     # Number of recurrent layers,or time steps, sometimes called w in textbooks\n",
    "sequence_length = 5\n",
    "batch_size = 3\n",
    "\n",
    "input_tensor = torch.randn(sequence_length, batch_size, input_size)\n",
    "h0 = torch.zeros(num_lstm_layers, batch_size, hidden_size)    # Initial hidden state, usually initiallized with zeros\n",
    "c0 = torch.zeros(num_lstm_layers, batch_size, hidden_size)    # Initial cell state, usually initiallized with zeros\n",
    "\n",
    "print(f'Input size: {input_size}, hidden size: {hidden_size}, number of recurrent layers: {num_lstm_layers}, batch size: {batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSTM class\n",
    "See [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). This class works on entire LSTM RNN and evaluate it over anhy input with specified number of sequence items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = torch.nn.LSTM(input_size, hidden_size, num_lstm_layers)\n",
    "\n",
    "output_tensor_lstm, (hn, cn) = lstm(input_tensor, (h0,c0))\n",
    "print(f'Shape of output tensor [{sequence_length} X {batch_size} X {hidden_size}]: {output_tensor_lstm.shape}')\n",
    "print(f'Shape of hidden-cell-states [{h0.shape}]: {hn.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMCell class\n",
    "See [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html). This class just encapsulates a single LSTM cell so to compute the forward pass over a network, we need to iterate. The example below is for a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell = torch.nn.LSTMCell(input_size, hidden_size)\n",
    "\n",
    "hx = torch.zeros(batch_size, hidden_size)    # Initial hidden state, usually initiallized with zeros\n",
    "cx = torch.zeros(batch_size, hidden_size)    # Initial cell state, usually initiallized with zeros\n",
    "\n",
    "print(input_tensor.size()[0])\n",
    "\n",
    "# hn, cn : hidden and cell states at last time step\n",
    "output_list = []\n",
    "for i in range(input_tensor.size()[0]):\n",
    "    # go over the input sequence\n",
    "    hx, cx = lstm_cell(input_tensor[i], (hx, cx))\n",
    "    output_list.append(hx)\n",
    "output_tensor_lstm_cell = torch.stack(output_list, dim=0)\n",
    "\n",
    "print(f'Shape of output tensor [{sequence_length} X {batch_size} X {hidden_size}]: {output_tensor_lstm_cell.shape}')\n",
    "print(f'Shape of hidden-cell-states [{batch_size} X {hidden_size}]: {hx.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = 'cpu'\n",
    "DEVICE = torch.device(device)\n",
    "\n",
    "TEXT_PORTION_SIZE = 200   # Number of time steps for each input in a batch\n",
    "EMBEDDING_DIM = 120       # dimension of embedding layer\n",
    "HIDDEN_DIM = 130          # dimesion of hidden layer\n",
    "TEXT_VOCAB_SIZE = len(string.printable)\n",
    "\n",
    "NUM_ITER = 5000\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_HIDDEN_LAYERS = 1\n",
    "\n",
    "print('Device:', DEVICE)\n",
    "print(f'Size of character vocabulary: {TEXT_VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "# divide test into small portions\n",
    "random.seed(RANDOM_SEED)\n",
    "def random_portion(textfile):\n",
    "    start_index = random.randint(0, TEXT_LENGTH - TEXT_PORTION_SIZE)\n",
    "    end_index = start_index + TEXT_PORTION_SIZE + 1\n",
    "    return textfile[start_index:end_index]\n",
    "\n",
    "# convert characters into tensors of integers (type long). The integer is\n",
    "# the position of the character in string.printable\n",
    "def char_to_tensor(text):\n",
    "    lst = [string.printable.index(c) for c in text]\n",
    "    tensor = torch.tensor(lst).long()\n",
    "    return tensor\n",
    "\n",
    "# Draw random sample for training: \n",
    "#     (1) split randomly the text \n",
    "#     (2) input is the random text from first character to one before last\n",
    "#     (3) target is the random text from second character to last character\n",
    "def draw_random_sample(textfile):  \n",
    "    text_long = char_to_tensor(random_portion(textfile))\n",
    "    inputs = text_long[:-1]\n",
    "    targets = text_long[1:]\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "with open('covid19-faq.txt', 'r') as f:\n",
    "    textfile = f.read()\n",
    "\n",
    "# convert special characters\n",
    "textfile = unidecode.unidecode(textfile)\n",
    "\n",
    "# strip extra whitespaces\n",
    "textfile = re.sub(' +',' ', textfile)\n",
    "\n",
    "TEXT_LENGTH = len(textfile)\n",
    "\n",
    "print(f'Printable characters: {string.printable} of size {len(string.printable)}')\n",
    "print(f'Number of characters in text: {TEXT_LENGTH}')\n",
    "print(f'Convert characters to tensor: {char_to_tensor(\"abcDEF\")}')\n",
    "print(f'A random portion of the textfile: {random_portion(textfile)}')\n",
    "\n",
    "input, target = draw_random_sample(textfile)\n",
    "print(f'NUmber of time steps in a training example={len(input)} [{input[0:5]}], target size={len(target)} [{target[0:5]}]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocabulary_size, embed_size,\n",
    "                 hidden_size, output_size):\n",
    "        \"\"\"Basic RNN model\n",
    "\n",
    "        Args:\n",
    "            input_size (_type_): dimension of a single input vector,\n",
    "            embed_size (_type_): dimension of embedding vector\n",
    "            hidden_size (_type_): _description_\n",
    "            output_size (_type_): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        #  num_embeddings : size of the dictionary of embeddings,  \n",
    "        #  embedding_dim  : the size of each embedding vector\n",
    "        # weights are trainable\n",
    "        self.embed = torch.nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embed_size) \n",
    "\n",
    "        self.rnn = torch.nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, character, hidden, cell_state):\n",
    "        # expects character as size [batch_size, 1]\n",
    "    \n",
    "        # [batch size, embedding dim] = [1, embedding dim]\n",
    "        embedded = self.embed(character)\n",
    "\n",
    "        (hidden, cell_state) = self.rnn(embedded, (hidden, cell_state))\n",
    "        # 1. output dim: [batch size, output_size] = [1, output_size]\n",
    "        # 2. hidden dim: [batch size, hidden dim] = [1, hidden dim]\n",
    "        # 3. cell dim: [batch size, hidden dim] = [1, hidden dim]\n",
    "\n",
    "        output = self.fc(hidden)\n",
    "\n",
    "        return output, hidden, cell_state\n",
    "      \n",
    "    def init_zero_state(self):\n",
    "        init_hidden = torch.zeros(1, self.hidden_size).to(DEVICE)\n",
    "        init_cell = torch.zeros(1, self.hidden_size).to(DEVICE)\n",
    "        return (init_hidden, init_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "vocabulary_size = TEXT_VOCAB_SIZE\n",
    "output_size = TEXT_VOCAB_SIZE\n",
    "\n",
    "model = RNN(vocabulary_size, EMBEDDING_DIM, HIDDEN_DIM, output_size)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    ## based on https://github.com/spro/practical-pytorch/\n",
    "    ## blob/master/char-rnn-generation/char-rnn-generation.ipynb\n",
    "\n",
    "    (hidden, cell_state) = model.init_zero_state()\n",
    "    print(hidden.shape, cell_state.shape)\n",
    "    prime_input = char_to_tensor(prime_str)    # transform \n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    # Run over characters of `prime_str` and run in the RNN\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        inp = prime_input[p].unsqueeze(0)   # tensor of size 1 containing the index of the character in the vocabulary\n",
    "        _, hidden, cell_state = model(inp.to(DEVICE), hidden, cell_state)\n",
    "    inp = prime_input[-1].unsqueeze(0)\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "\n",
    "        outputs, hidden, cell_state = model(inp.to(DEVICE), hidden, cell_state)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        # The higher is the temprature, the more diverse output will be generated at each evaluate (with same input string)\n",
    "        output_dist = outputs.data.view(-1).div(temperature).exp() # e^{logits / T}  - logits on the set of 100 characters\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]    # sample from a multinomial distribution with probablities determioned by th elogits output of the network\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = string.printable[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_to_tensor(predicted_char)\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp = evaluate(model, prime_str='The', temperature=0.8)\n",
    "print(ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for iteration in range(NUM_ITER):\n",
    "\n",
    "    hidden, cell_state = model.init_zero_state()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0.\n",
    "    inputs, targets = draw_random_sample(textfile)\n",
    "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "    \n",
    "    for c in range(TEXT_PORTION_SIZE):\n",
    "        # Run over all characters in the random ranple. unsqueeze adds empty dimension\n",
    "        outputs, hidden, cell_state = model(inputs[c].unsqueeze(0), hidden, cell_state)\n",
    "        loss += torch.nn.functional.cross_entropy(outputs, targets[c].view(1))\n",
    "\n",
    "    loss /= TEXT_PORTION_SIZE\n",
    "    loss.backward()\n",
    "    \n",
    "    ### UPDATE MODEL PARAMETERS\n",
    "    optimizer.step()\n",
    "\n",
    "    ### LOGGING\n",
    "    with torch.no_grad():\n",
    "        if iteration % 200 == 0:\n",
    "            print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "            print(f'Iteration {iteration} | Loss {loss.item():.2f}\\n\\n')\n",
    "            print(evaluate(model, 'Th', 200), '\\n')\n",
    "            print(50*'=')\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "            plt.clf()\n",
    "            plt.plot(range(len(loss_list)), loss_list)\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Iteration x 1000')\n",
    "            plt.savefig('loss1.pdf')\n",
    "            \n",
    "plt.clf()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iteration x 1000')\n",
    "plt.plot(range(len(loss_list)), loss_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b490ba79cd6e0cad5d561930ddcb592f66007d6828f74aef7cec19c0ea8e73d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dvlp_m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
