{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the blog/notebook [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/#background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "from os.path import exists\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import spacy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "# import GPUtil\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "\n",
    "# import altair as alt\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True\n",
    "\n",
    "import platform\n",
    "print(platform.platform())\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self attention\n",
    "\n",
    "### Single head attention\n",
    "Maps a set $(x_1,x_2, \\ldots, x_T) \\in \\mathbb{R}^{d_k}$ to a set $(z_1,z_2, \\ldots, z_T) \\in \\mathbb{R}^{d_v}$. The mapping is parametrized by three linear operators (matrics):\n",
    "- `key`: $W_k \\in \\mathbb{R}^{d_e \\times d_k}$ transform a token $x_i$ to a **key** view $k_i =W_kx_i \\in \\mathbb{R}^{d_e}$\n",
    "- `query` : $W_q \\in \\mathbb{R}^{d_e \\times d_k}$ transform a token $x_i$ to a **query** view $q_i =W_qx_i  \\in \\mathbb{R}^{d_e}$\n",
    "- `value`: $W_v$ transform a token $x_i$ to a **value** view $v_i =W_vx_i  \\in \\mathbb{R}^{d_v}$\n",
    "$$\n",
    " z_i = \\sum_{j=1}^n p_i(j)v_j \\;,\\; p_i(j) = \\frac{\\exp( <q_i, k_j>)}{Z_i}\n",
    "$$\n",
    "that cab be represented in matrix form as\n",
    "$$\n",
    "Z = \\tt{softmax}(Q \\cdot K^T) V\n",
    "$$\n",
    "In practice, we use a `scaled` version (see [the blog](http://nlp.seas.harvard.edu/annotated-transformer/#background) for explanation)\n",
    "$$\n",
    "Z = \\tt{softmax}(\\frac{Q \\cdot K^T}{\\sqrt{d}}) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "# H = 1      # number of heads\n",
    "# T =3       # Number of time steps \n",
    "# d_e = 4,   # dimension of the input embedding vectors\n",
    "# d_k = 2    # dimension of key and query vectors\n",
    "# d_v = d_e / H\n",
    "\n",
    "H = 1\n",
    "T = 3\n",
    "d_e = 4\n",
    "d_k = 2\n",
    "d_v = 5 \n",
    "\n",
    "x_1 = torch.randn([d_e])\n",
    "x_2 = torch.randn([d_e])\n",
    "x_3 = torch.randn([d_e])\n",
    "print(x_1.shape, x_2.shape, x_3.shape)\n",
    "X = torch.stack([x_1,x_2,x_3])\n",
    "print(f'X shape: {X.shape} [{T} X {d_e}]')\n",
    "\n",
    "W_k = torch.randn((d_k, d_e))\n",
    "\n",
    "k_1 = torch.matmul(W_k, x_1)\n",
    "k_2 = torch.matmul(W_k, x_2)\n",
    "k_3 = torch.matmul(W_k, x_3)\n",
    "K_manual = torch.stack([k_1,k_2,k_3])\n",
    "K = torch.matmul(X, W_k.T)              # same as K = torch.matmul(W_k, X.T).T\n",
    "\n",
    "print(f'K_manual shape: {K_manual.shape} [{T} X {d_k}]')\n",
    "print(f'K shape: {K.shape}   [err={torch.linalg.norm(K-K_manual)}]')\n",
    "\n",
    "W_q = torch.randn((d_k, d_e))\n",
    "Q = torch.matmul(X, W_q.T)            \n",
    "print(f'Q shape: {Q.shape}  [{T} X {d_k}]')\n",
    "\n",
    "scaled_dot_prod_attention = torch.matmul(Q, K.T) / math.sqrt(d_k)\n",
    "print(f'dot-product attention shape: {scaled_dot_prod_attention.shape}  [{T} X {T}]')\n",
    "\n",
    "# Do the softmax to get in each row, a distribution \n",
    "sfmx = scaled_dot_prod_attention.softmax(dim=-1)\n",
    "print(f'Number of rows: {sfmx.shape[0]}, sums each rows={torch.sum(sfmx, dim=-1)}')\n",
    "\n",
    "W_v = torch.randn((d_v, d_e))\n",
    "V = torch.matmul(X, W_v.T)            \n",
    "print(f'V shape: {V.shape}  [{T} X {d_v}]')\n",
    "\n",
    "# # munliply scores with values to get output tokens\n",
    "Z = sfmx @  V\n",
    "print(f'Shape of output tokens: {Z.shape} [{T} X {d_v}]')\n",
    "\n",
    "\n",
    "#  # vectorized representation\n",
    "# V = X @ W_v\n",
    "# print(f'key reresentation for x  :\\n {K}')       # vectorized representation\n",
    "\n",
    "# print(f'qry reresentation for x  :\\n {Q}')      \n",
    "# print(f'val reresentation for x  :\\n {V}')       # vectorized representation\n",
    "\n",
    "# # compute (dot-product) attention scores\n",
    "# dot_prod_attention = (Q @ K.T) / torch.sqrt(d)\n",
    "# sfmx = dot_prod_attention.softmax(dim=-1)\n",
    "# print(f'all attention scores:\\n  {dot_prod_attention}')\n",
    "# print(f'Attention scores for x_1: {dot_prod_attention[0,:]}')\n",
    "\n",
    "# print(f'V = {V}')\n",
    "# print(f'softmaxing: {sfmx}')\n",
    "\n",
    "# # munliply scores with values to get output tokens\n",
    "# Z = sfmx @  V\n",
    "# print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_1 = torch.tensor([1.0,1.0,1.0,1.0])\n",
    "xx_2 = torch.tensor([2.0,2.0,2.0,2.0])\n",
    "xx_3 = torch.tensor([3.0,3.0,3.0,3.0])\n",
    "XX = torch.stack([xx_1,xx_2,xx_3])\n",
    "print(XX.shape)\n",
    "print(f'sums each rows={torch.sum(XX, dim=-1)}')\n",
    "print(f'sums each colimn={torch.sum(XX, dim=0)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        print(f'Input tensor: sequence length={x.shape[0]}, batch_size={x.shape[1]}, embedding dimension={x.shape[2]}')\n",
    "        print(f'Shape of pe: {self.pe.shape}')\n",
    "        x_add = self.pe[:x.size(0), :]\n",
    "        print(f'add to input: {x_add.shape}')\n",
    "        print(f'add to first token: {x_add[0,0,:]}')\n",
    "        print(f'add to first token: {x_add[1,0,:]}')\n",
    "        print(f'add to third token: {x_add[2,0,:]}')\n",
    "        x = x + x_add\n",
    "        return x\n",
    "\n",
    "p_enc = PositionalEncoding(d_e)\n",
    "\n",
    "x_in = torch.ones((T, 32, d_e))\n",
    "x_w_p_enc = p_enc(x_in)\n",
    "print(x_in.shape, x_w_p_enc.shape)\n",
    "# print(x_in)\n",
    "# print(x_w_p_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    print(query.size(), d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # scores = torch.matmul(query, key.T) # / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "print(Q.shape)\n",
    "Z1, patt = attention(Q, K, V)\n",
    "print(torch.norm(Z1-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir):\n",
    "  # model_dir = Path(model_dir)/\"saved_model\"\n",
    "  # model_dir = Path(model_dir)\n",
    "  # print('Model loaded to: {}'.format(model_dir))\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    "  model = model.signatures['serving_default']\n",
    "\n",
    "  return model\n",
    "\n",
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                    for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = '/opt/models/nexserve/constructions_v4_rfcn_high_res_b/1'\n",
    "saved_model = tf.saved_model.load(saved_model_dir)\n",
    "saved_model = saved_model.signatures['serving_default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = '/opt/datasets/cones.jpg'\n",
    "image_np = np.array(Image.open(input_image))\n",
    "output_dict = run_inference_for_single_image(saved_model, image_np)\n",
    "\n",
    "visualize_boxes_and_labels_on_image_array(\n",
    "    image_np,\n",
    "    output_dict['detection_boxes'],\n",
    "    output_dict['detection_classes'],\n",
    "    output_dict['detection_scores'],\n",
    "    use_normalized_coordinates=True,\n",
    "    min_score_thresh=0.3,\n",
    "    line_thickness=8)\n",
    "\n",
    "\n",
    "\n",
    "display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(output_dict.keys())\n",
    " print(output_dict['detection_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_model_dir)\n",
    "!saved_model_cli show --dir $saved_model_dir --tag_set serve --signature_def serving_default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "\n",
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
    "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
    "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
    "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
    "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
    "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
    "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
    "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
    "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
    "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
    "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color='red',\n",
    "                               thickness=4,\n",
    "                               display_str_list=(),\n",
    "                               use_normalized_coordinates=True):\n",
    "\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "  try:\n",
    "    font = ImageFont.truetype('arial.ttf', 24)\n",
    "  except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle(\n",
    "        [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n",
    "                                                          text_bottom)],\n",
    "        fill=color)\n",
    "    draw.text(\n",
    "        (left + margin, text_bottom - text_height - margin),\n",
    "        display_str,\n",
    "        fill='black',\n",
    "        font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "def draw_bounding_box_on_image_array(\n",
    "  image,\n",
    "  ymin,\n",
    "  xmin,\n",
    "  ymax,\n",
    "  xmax,\n",
    "  color='red',\n",
    "  thickness=4,\n",
    "  display_str_list=(),\n",
    "  use_normalized_coordinates=True):\n",
    "\n",
    "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color,\n",
    "                            thickness, display_str_list,\n",
    "                            use_normalized_coordinates)\n",
    "  np.copyto(image, np.array(image_pil))\n",
    "\n",
    "def visualize_boxes_and_labels_on_image_array(\n",
    "    image,\n",
    "    boxes,\n",
    "    classes,\n",
    "    scores,\n",
    "    use_normalized_coordinates=False,\n",
    "    max_boxes_to_draw=20,\n",
    "    min_score_thresh=.5,\n",
    "    agnostic_mode=False,\n",
    "    line_thickness=4,\n",
    "    groundtruth_box_visualization_color='black',\n",
    "    skip_scores=False,\n",
    "    skip_labels=True):\n",
    "\n",
    "  # Create a display string (and color) for every box location, group any boxes\n",
    "  # that correspond to the same location.\n",
    "  box_to_display_str_map = collections.defaultdict(list)\n",
    "  box_to_color_map = collections.defaultdict(str)\n",
    "  box_to_instance_masks_map = {}\n",
    "  box_to_instance_boundaries_map = {}\n",
    "  box_to_keypoints_map = collections.defaultdict(list)\n",
    "  if not max_boxes_to_draw:\n",
    "    max_boxes_to_draw = boxes.shape[0]\n",
    "  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "    if scores is None or scores[i] > min_score_thresh:\n",
    "      box = tuple(boxes[i].tolist())\n",
    "      if scores is None:\n",
    "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
    "      else:\n",
    "        display_str = ''\n",
    "        # if not skip_labels:\n",
    "        #   if not agnostic_mode:\n",
    "        #     if classes[i] in category_index.keys():\n",
    "        #       class_name = category_index[classes[i]]['name']\n",
    "        #     else:\n",
    "        #       class_name = 'N/A'\n",
    "        #     display_str = str(class_name)\n",
    "        if not skip_scores:\n",
    "          if not display_str:\n",
    "            display_str = '{}%'.format(int(100*scores[i]))\n",
    "          else:\n",
    "            display_str = '{}: {}%'.format(display_str, int(100*scores[i]))\n",
    "        box_to_display_str_map[box].append(display_str)\n",
    "        if agnostic_mode:\n",
    "          box_to_color_map[box] = 'DarkOrange'\n",
    "        else:\n",
    "          box_to_color_map[box] = STANDARD_COLORS[\n",
    "              classes[i] % len(STANDARD_COLORS)]\n",
    "\n",
    "  # Draw all boxes onto image.\n",
    "  for box, color in box_to_color_map.items():\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    draw_bounding_box_on_image_array(\n",
    "        image,\n",
    "        ymin,\n",
    "        xmin,\n",
    "        ymax,\n",
    "        xmax,\n",
    "        color=color,\n",
    "        thickness=line_thickness,\n",
    "        display_str_list=box_to_display_str_map[box],\n",
    "        use_normalized_coordinates=use_normalized_coordinates)\n",
    "\n",
    "  return image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b490ba79cd6e0cad5d561930ddcb592f66007d6828f74aef7cec19c0ea8e73d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dvlp_m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
